{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Notes for me:\n",
    "- Remove always show toolbar before starting\n",
    "- Set zoom to 150%\n",
    "- check there is internet\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with Python and Scikit Learn\n",
    "\n",
    "Follow presentation on: http://bit.ly/ML-SpringCampus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?\n",
    "\n",
    "The process of teaching computers to learn from data.\n",
    "\n",
    "### Learning tasks:\n",
    "\n",
    "- Clustering\n",
    "\n",
    "- Regression\n",
    "\n",
    "- Outlier Detection\n",
    "\n",
    "- Classification\n",
    "\n",
    "- Time series prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised vs Unsupervised Learning\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## Classification vs Regression\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at some code:\n",
    "\n",
    "We will use:\n",
    " - [Pandas: Python Data Analysis Library](http://pandas.pydata.org/)\n",
    " - [Scikit-Learn](http://scikit-learn.org/stable/)\n",
    " - [Numpy](http://www.numpy.org/)\n",
    " - [Matplotlib](matplotlib.org)\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "__Importing stuff...__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "import Utils\n",
    "from Utils import cmap_light\n",
    "from Utils import cmap_bold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "# Boston House Prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "boston_dataset = datasets.load_boston()\n",
    "print(boston_dataset.DESCR)\n",
    "\n",
    "X = boston_dataset.data\n",
    "Y = boston_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = list(boston_dataset.feature_names) + ['Price']\n",
    "\n",
    "labels = np.reshape(Y,\n",
    "                     (Y.shape[0], 1))\n",
    "df = pd.DataFrame(data=np.concatenate((X, labels), axis=1),\n",
    "                 columns=names)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "## The data\n",
    "\n",
    "\n",
    "__Dataset:__ \n",
    "\n",
    "a set of __examples__ each characterized by __features__ and  usually there is a __label variable__ that you want to predict. \n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "## Let's see a real example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tmp = df[['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX',\n",
    "             'RM', 'Price']]\n",
    "df_tmp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tmp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Utils import plot_boston_dataset\n",
    "plot_boston_dataset(boston_dataset.data, \n",
    "                    boston_dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "## First model:  Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "<img src=\"https://imgs.xkcd.com/comics/linear_regression.png\" title=\"XKCD: Linear Regression\" width=\"512\" height=\"342\" />\n",
    "\n",
    "_The 95% confidence interval suggests Rexthor's dog could also be a cat, or possibly a teapot._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X, Y)\n",
    "\n",
    "r2 = model.score(X, Y)\n",
    "\n",
    "print(\"R^2 value: {:0.3f}\".format(r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_n = np.random.randint(0, Y.shape[0])\n",
    "\n",
    "Utils.describe_example_boston_dataset(X[example_n])\n",
    "\n",
    "print(\"\\n\\nPredicted price: {:2.2f} Real value: {:2.2f}\".format(\n",
    "        model.predict(X[example_n].reshape(1, -1))[0], Y[example_n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "## Distinguishing Species of Iris plants:\n",
    "\n",
    "<br><br>\n",
    "<img src=\"https://c1.staticflickr.com/1/402/31713322286_8c85f8b6b6_z.jpg\" title=\"Iris flower\" width=\"512\" height=\"342\" />\n",
    "\n",
    "[Source: Big Cypress National Preserve](https://www.flickr.com/photos/bigcypressnps/31713322286/in/photolist-Qjp4QJ-ha3kMy-4KVwB4-4Minkn-fb7Jye-sb5tXz-dsWnJ9-4LhUuW-6oioBi-4LhUdA-GFsivk-szxsRT-dkse9b-4KVwk4-GPNUCc-dksd7u-ha6uis-dtXQmu-2enBH-qsaLo7-qsaUYy-rp5XxE-rp9N4p-r7JFac-r5SjCe-dksdro-dkscBm-HKf4AB-LcBBEv-dksbkv-dksb2i-dksa8H-dksbDi-rRTCUf-wRiRYq-dksd3N-dkscSd-dkscP5-dksag6-a2hYQC-e26B2-r7AuaS-rp6h47-2enc6-r7CpdL-r7H5uH-r7C435-rmT8D1-r7BefQ-J4WHV)\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_dataset = datasets.load_iris()\n",
    "\n",
    "print(\"Features: \" + str(iris_dataset.feature_names))\n",
    "print(\"Classes: \" + str(iris_dataset.target_names))\n",
    "\n",
    "X = iris_dataset.data\n",
    "y = iris_dataset.target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put them in a random order\n",
    "idx = np.random.permutation(150)\n",
    "y = y[idx]\n",
    "X = X[idx]\n",
    "\n",
    "labels = np.reshape(y,\n",
    "                    (y.shape[0], 1))\n",
    "df = pd.DataFrame(data=np.concatenate((X, labels), axis=1),\n",
    "                 columns=iris_dataset.feature_names + ['Class'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's take a peak at the data:\n",
    "plt.figure(figsize=(8,8))\n",
    "colors = \"bry\"\n",
    "for i, color in zip([0, 1, 2], colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired)\n",
    "    \n",
    "plt.text(5.25, 2.20, \"Versicolor\", fontsize=14)\n",
    "plt.text(7, 3.5, \"Virginica\", fontsize=14)\n",
    "plt.text(4.5, 3.75, \"Setosa\", fontsize=14)\n",
    "\n",
    "plt.title(\"The 3 different Iris species\", fontsize=18, \n",
    "          fontweight='bold')    \n",
    "plt.xlabel(iris_dataset.feature_names[0], fontsize=14)\n",
    "plt.ylabel(iris_dataset.feature_names[1], fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will focus identifying only the Iris Setosa\n",
    "plt.figure(figsize=(8,8))\n",
    "colors = \"br\"\n",
    "\n",
    "idx = np.where(y == 0) # Give me the indices of the Iris Setosa examples\n",
    "\n",
    "plt.scatter(X[idx, 0], X[idx, 1], c='b', cmap=plt.cm.Paired)\n",
    "plt.text(4.5, 3.75, \"Setosa\", fontsize=14)\n",
    "\n",
    "idx = np.where(y != 0) # where it's not Iris Setosa \n",
    "plt.scatter(X[idx, 0], X[idx, 1], c='r', cmap=plt.cm.Paired)\n",
    "plt.text(7.0, 2.5, \"Others\", fontsize=14)\n",
    "\n",
    "\n",
    "plt.title(\"Scatter plot of Iris Setosa and the others Iris\",\n",
    "          fontsize=18, fontweight='bold')  \n",
    "plt.xlabel(iris_dataset.feature_names[0], fontsize=14)\n",
    "plt.ylabel(iris_dataset.feature_names[1], fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "### Second model  Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We only care about whether each flower is a \n",
    "#     Iris Setosa and we are looking only at two of their features\n",
    "\n",
    "X = iris_dataset.data\n",
    "y = iris_dataset.target\n",
    "\n",
    "new_y = y == 0\n",
    "\n",
    "model = LogisticRegression(random_state=42, verbose=0)\n",
    "\n",
    "model.fit(X[:,0:2], new_y)\n",
    "\n",
    "accuracy = model.score(X[:,0:2], new_y)\n",
    "\n",
    "print(\"Accuracy: {:0.3f}%\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Utils import predict_mesh\n",
    "\n",
    "# Let's take a look at what our model is doing\n",
    "\n",
    "# First plot the examples\n",
    "plt.figure(figsize=(8,8))\n",
    "colors = \"br\"\n",
    "\n",
    "idx = np.where(y == 0)\n",
    "plt.scatter(X[idx, 0], X[idx, 1], c='b', cmap=plt.cm.Paired)\n",
    "plt.text(4.5, 3.75, \"Setosa\", fontsize=14)\n",
    "\n",
    "idx = np.where(y != 0)\n",
    "plt.scatter(X[idx, 0], X[idx, 1], c='r', cmap=plt.cm.Paired)\n",
    "plt.text(7.0, 2.5, \"Others\", fontsize=14)\n",
    "\n",
    "(xx, yy, Z) = predict_mesh(X, model)\n",
    "plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "\n",
    "plt.title(\"Decision Boundary\", fontsize=18, fontweight='bold')   \n",
    "plt.xlabel(iris_dataset.feature_names[0], fontsize=14)\n",
    "plt.ylabel(iris_dataset.feature_names[1], fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## Linear Regression and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So how do these models work?\n",
    "\n",
    "Let's start with linear regression:\n",
    "\n",
    "$$ \\hat{y} = w_0 + w_1.x_1 + w_2.x_2 + w_3.x_3$$ \n",
    "\n",
    "Adding a $x_0=1$ we get\n",
    "\n",
    "\n",
    "$$ \\hat{y} = w^T \\cdot x $$\n",
    "\n",
    "For each variable we have a weight, an \"importance\", and the linear combination of the weights and features results in our estimated value $\\hat{y}$.\n",
    "\n",
    "# Questions?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "#### How do we find the weights?\n",
    "\n",
    "It's an optimization problem we want to find the weights that minimize the error over all the examples we have for training.\n",
    "\n",
    "We can use any kind of optimization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "We want to use the same linear model  but now build a classifier.\n",
    "\n",
    "\n",
    "For that we use the Logistic/Signmoid function .\n",
    "\n",
    "taking:\n",
    "\n",
    "$$ \\hat{y} = w^T \\cdot x $$\n",
    "\n",
    "\n",
    "$$ \\hat{y} = g(w^T \\cdot x) $$\n",
    "\n",
    "where: \n",
    "\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.arange(-20, 20, 0.001)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Sigmoid Function\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Now to a real world example! \n",
    "\n",
    "Let's take the knowledge we gained and try to apply it to a real world dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br> \n",
    "# Scotch!\n",
    "![Scotch whiskies](https://c1.staticflickr.com/7/6184/6105844311_dc4c31b8b7_b.jpg)\n",
    "[Source: Damien Pollet](https://flic.kr/p/aiy3MM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "# First look at the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the data file and drop the collumns we don't care about:\n",
    "whisky_dataframe = pd.read_csv(\n",
    "    filepath_or_buffer=\"whiskies.csv\", header=0, sep=',',\n",
    "    index_col=1)\n",
    "whisky_dataframe.drop(['RowID', 'Postcode', ' Latitude',\n",
    "                       ' Longitude'], inplace=True, axis=1)\n",
    "whisky_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whisky_dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Utils.plot_whisky_histograms(whisky_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Utils.plot_whiky_body_correlation(whisky_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whisky_data = whisky_dataframe.values\n",
    "whisky_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## Overfitting\n",
    "### The more data you have the better? Not always:\n",
    "\n",
    "__ The curse of dimentionality__\n",
    " \n",
    " \n",
    " _ When the dimensionality increases, the volume of the space increases so fast that the available data become sparse. _\n",
    " \n",
    " \n",
    "__ Model Complexity__\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "#### Feature selection and extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_data = np.random.randn(100, 2)\n",
    "random_labels = np.random.randint(0,2,100)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(random_data[:, 0], random_data[:, 1],\n",
    "            c=random_labels, cmap=cmap_bold)\n",
    "\n",
    "plt.xlabel('Random Dimension 1', fontsize=14)\n",
    "plt.ylabel('Random Dimension 2', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(random_data, random_labels)\n",
    "\n",
    "print(\"Accuracy: {:0.3f}%\".format(\n",
    "        clf.score(random_data, random_labels)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(xx, yy, Z) = predict_mesh(random_data, clf, h=0.01)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.xlabel('Random Dimension 1', fontsize=14)\n",
    "plt.ylabel('Random Dimension 2', fontsize=14)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "plt.scatter(random_data[:, 0], random_data[:, 1],\n",
    "            c=random_labels, cmap=cmap_bold)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "## The curse of Dimensionality\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "__ More features != Better Results __\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_labels = np.concatenate([np.ones((50,)), np.zeros((50,))])\n",
    "\n",
    "random_data = np.concatenate([\n",
    "    np.add(np.multiply(np.random.randn(50, 2), \n",
    "                       np.array([0.7, 1.5])), np.array([3, 1])),\n",
    "    np.multiply(np.random.randn(50, 2), \n",
    "                np.array([0.5, 3]))\n",
    "    ]) \n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.scatter(random_data[:, 0], random_data[:, 1], \n",
    "            c=random_labels, cmap=cmap_bold)\n",
    "plt.xlim((-4, 8))\n",
    "plt.ylim((-6, 6))\n",
    "plt.xlabel('Random Dimension 1', fontsize=14)\n",
    "plt.ylabel('Random Dimension 2', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## PCA\n",
    "\n",
    "Principal componentes are the directions of largest variance\n",
    "\n",
    "The eigenvectors with the largest eigenvalues are the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_data =np.random.multivariate_normal(\n",
    "    mean= [0, 0], cov=[[5, 5], [0, 0.5]], size=100)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.scatter(random_data[:, 0], random_data[:, 1])\n",
    "plt.plot([-5, 5], [-5, 5], 'r--')\n",
    "plt.plot([1.5, -1.5], [-1.5, 1.5], 'g--')\n",
    "\n",
    "plt.xlim((-7, 7))\n",
    "plt.ylim((-7, 7))\n",
    "plt.title('Random Data with Principal Components', fontsize=16)\n",
    "\n",
    "plt.xlabel('Random Dimension 1', fontsize=14)\n",
    "plt.ylabel('Random Dimension 2', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(random_data)\n",
    "transformed_data = pca.fit_transform(random_data)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(transformed_data[:,0], transformed_data[:,1])\n",
    "plt.plot([-5, 5], [0, 0], 'r--')\n",
    "plt.xlim((-7, 7))\n",
    "plt.ylim((-7, 7))\n",
    "plt.title('Transformed Random Data', fontsize=16)\n",
    "plt.xlabel('Random Dimension 1', fontsize=14)\n",
    "plt.ylabel('Random Dimension 2', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Back to Scotch!\n",
    "\n",
    "Let's apply what we learned to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, whiten=True) \n",
    "# Here whiten means centering the data around 0, \n",
    "# which is needed so that PCA works correctly\n",
    "transformed_data = pca.fit_transform(whisky_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"% of variance explained by each component: 1st {:0.1f}% 2nd {:0.1f}%\".format(\n",
    "        pca.explained_variance_ratio_[0]*100, pca.explained_variance_ratio_[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(x = transformed_data[:,0], y=transformed_data[:,1])\n",
    "\n",
    "plt.xlim((-3, 5))\n",
    "plt.ylim((-3, 5))\n",
    "\n",
    "plt.title('Transformed Whisky Data', fontsize=16)\n",
    "plt.xlabel('Principal Component 1', fontsize=14)\n",
    "plt.ylabel('Principal Component 2', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "### Predicting whether it has Tobacco taste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = whisky_dataframe['Tobacco']\n",
    "whisky_data = whisky_dataframe.drop('Tobacco', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Percentage of Positive Labels: {:.2f}%\".format(\n",
    "        np.sum(labels)/len(labels)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weight={0:1, 1: 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, whiten=True) \n",
    "# Here whiten means centering the data around 0, \n",
    "# which is needed so that PCA works correctly\n",
    "transformed_data = pca.fit_transform(whisky_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classf = LogisticRegression(random_state=42, \n",
    "                            class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select parameters to use in Cross-Validation\n",
    "classf_cv = classf\n",
    "data_cv = transformed_data\n",
    "N_CV = 10\n",
    "\n",
    "# Cross Validation\n",
    "t0 = time()\n",
    "scores = cross_val_score(classf_cv, data_cv, labels, cv = N_CV)\n",
    "print(\"Scores: \")\n",
    "for i, score in enumerate(scores):\n",
    "    print( '\\t' + str(i) + ':\\t' + str(score)) \n",
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(\"\\nCross val done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    transformed_data, labels, test_size=0.20)\n",
    "\n",
    "classf = LogisticRegression(class_weight=class_weight)\n",
    "\n",
    "classf.fit(train_data, train_labels)\n",
    "\n",
    "predicted_labels = classf.predict(test_data)\n",
    "confusion_matrix(test_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "## Resources:\n",
    "\n",
    "#### Online Courses:\n",
    "- [Machine Learning by Andrew Ng](https://www.coursera.org/learn/machine-learning)\n",
    "\n",
    "#### Books:\n",
    "- [Master Algorithm](https://www.amazon.com/Master-Algorithm-Ultimate-Learning-Machine/dp/0465065708)\n",
    "- [Pattern Recognition and Machine Learning](https://www.amazon.co.uk/Pattern-Recognition-Learning-Information-Statistics-x/dp/0387310738)\n",
    "\n",
    "#### Articles:\n",
    "- [A few useful things to know about Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "\n",
    "\n",
    "#### Tutorials:\n",
    "- [Kaggle: Predicting Survival rate on the Titanic](https://www.kaggle.com/c/titanic)\n",
    "- [Scikit-Learn Tutorials](http://scikit-learn.org/stable/tutorial/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLTutorial",
   "language": "python",
   "name": "mltutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
